# ğŸŒ¾ Agricultural Sensor Data Pipeline

## ğŸ“Œ Overview
This project implements a **production-grade data pipeline** for agricultural sensor data as part of the **SDE-2 Data Engineering Assignment**.  
The pipeline ingests, transforms, validates, and stores **sensor readings** (soil moisture, temperature, humidity, light intensity, battery levels) into a **clean, enriched, and query-optimized dataset** for downstream analytics.



## ğŸ“‚ Project Structure
.
â”œâ”€â”€ artifacts/ # Stores generated reports (e.g., data_quality_report.csv)
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw input parquet files (daily sensor data)
â”‚ â”œâ”€â”€ processed/ # Processed + partitioned parquet files
â”œâ”€â”€ notebook/ # Jupyter notebooks for experiments/debugging
â”œâ”€â”€ src/ # Core pipeline code
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ components/ # Modular pipeline components
â”‚ â”‚ â”œâ”€â”€ data_ingestion.py
â”‚ â”‚ â”œâ”€â”€ data_transformation.py
â”‚ â”‚ â”œâ”€â”€ data_validation.py
â”‚ â”‚ â””â”€â”€ data_loading.py
â”‚ â”œâ”€â”€ logger.py # Logging utility
â”‚ â””â”€â”€ exception.py # Custom exception handling
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ setup.py # Package setup script
â”œâ”€â”€ Dockerfile # Container setup for reproducibility
â””â”€â”€ README.md # Project documentation

markdown
Copy code


## âš™ï¸ Features

### ğŸ”¹ Data Ingestion
- Reads **daily parquet files** from `data/raw/`.
- Supports **incremental loading** using file naming & timestamp checkpoints.
- Uses **DuckDB** for:
  - Schema inspection
  - Validation queries (e.g., missing columns, data ranges)
  - Ingestion statistics logging
- Handles:
  - Corrupt/unreadable files
  - Schema mismatches
  - Missing or invalid values

### ğŸ”¹ Data Transformation
- Cleans data:
  - Drops duplicates
  - Handles missing values
  - Detects & corrects outliers (z-score > 3)
- Enriches with derived fields:
  - Daily average per sensor & reading type
  - 7-day rolling average
  - `anomalous_reading = true/false`
  - Normalization (using calibration parameters)
- Timestamp standardization:
  - ISO 8601 format
  - Adjusted to **UTC+5:30**

### ğŸ”¹ Data Validation (DuckDB)
- Type & schema validation
- Value range checks per reading type
- Gap detection using `generate_series` (hourly coverage check)
- Profiling:
  - % missing values
  - % anomalies
  - Coverage gaps
- Exports results as:  
  ğŸ“„ `artifacts/data_quality_report.csv`

### ğŸ”¹ Data Loading & Storage
- Stores cleaned data in **partitioned parquet format** under `data/processed/`
- Optimized for analytics:
  - Partitioned by `date` & `sensor_id`
  - Columnar format + compression (`snappy`)

---

## ğŸ³ Docker Setup
Build and run the pipeline inside a container:


# Build the image
docker build -t agri-pipeline .

# Run the container
docker run -it --rm -v $(pwd):/app agri-pipeline
â–¶ï¸ Running the Pipeline
Local (Python)
bash
Copy code
# Install dependencies
pip install -r requirements.txt

# Run the pipeline
python src/components/data_ingestion.py
python src/components/data_transformation.py
python src/components/data_validation.py
python src/components/data_loading.py
Docker
bash
Copy code
docker build -t agri-pipeline .
docker run -it --rm agri-pipeline
ğŸ“ Example Output
Processed Data: Stored at data/processed/

Validation Report: Generated at artifacts/data_quality_report.csv

ğŸ“Š Sample Queries (DuckDB)
sql
Copy code
-- Inspect anomalies
SELECT * FROM read_parquet('data/processed/*/*.parquet')
WHERE anomalous_reading = TRUE;

-- Daily average temperature
SELECT date, sensor_id, AVG(value) AS avg_temp
FROM read_parquet('data/processed/*/*.parquet')
WHERE reading_type = 'temperature'
GROUP BY date, sensor_id;
ğŸ§ª Testing
Unit tests cover core logic:

Ingestion validation

Transformation (outlier detection, rolling average, normalization)

Validation (range checks, missing values, gaps)

Run tests:

bash
Copy code
pytest tests/
ğŸ“¦ Deliverables
âœ… Modular, production-grade data pipeline
âœ… Unit tests + exception handling
âœ… Dockerized setup for reproducibility
âœ… Example raw + processed data
âœ… Data quality report